{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vt00xbH1tsrU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "j6eCRf8hvGB4"
      },
      "outputs": [],
      "source": [
        "# Loading SpaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_data(data, show_samples=5):\n",
        "    \"\"\"Preprocess text data using SpaCy.\"\"\"\n",
        "    preprocessed_data = []\n",
        "    for i, doc in enumerate(nlp.pipe(data, disable=[\"ner\", \"parser\", \"attribute_ruler\"], batch_size=100)):\n",
        "        tokens = [token.text.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
        "        preprocessed_data.append(\" \".join(tokens))\n",
        "\n",
        "        # Show a few samples for debugging\n",
        "        if i < show_samples:\n",
        "            print(f\"Original Text: {data[i][:200]}\")\n",
        "            print(f\"Preprocessed Text: {preprocessed_data[-1]}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    return preprocessed_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2qI45ajKvNqJ",
        "outputId": "b4c9a2d1-6593-4ba6-d3a4-ca9abe954fc5"
      },
      "outputs": [],
      "source": [
        "# Loading dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "X = newsgroups.data[:5000]  \n",
        "y = newsgroups.target[:5000]\n",
        "\n",
        "# Preprocessing the text data\n",
        "print(\"Preprocessing the dataset...\")\n",
        "X_processed = preprocess_data(X, show_samples=3)\n",
        "\n",
        "# Tokenizing and paddding sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=20000)  \n",
        "tokenizer.fit_on_texts(X_processed)\n",
        "X_sequences = tokenizer.texts_to_sequences(X_processed)\n",
        "X_pad = pad_sequences(X_sequences, maxlen=100) \n",
        "\n",
        "# Training and testing split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Encoding target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "\n",
        "num_classes = len(np.unique(y_train))\n",
        "print(f\"Dataset ready. X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nBBz5VvIwNjm"
      },
      "outputs": [],
      "source": [
        "# Defining configurations for different GRU depths\n",
        "shallow_config = {'gru_layers': 1, 'hidden_units': 16, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'epochs': 5}\n",
        "medium_config = {'gru_layers': 2, 'hidden_units': 32, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'epochs': 10}\n",
        "deep_config = {'gru_layers': 3, 'hidden_units': 64, 'dropout_rate': 0.4, 'learning_rate': 0.001, 'epochs': 15}\n",
        "\n",
        "all_configs = [shallow_config, medium_config, deep_config]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8laVZCtCwRLW"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(config, activation_func):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=20000, output_dim=128)) \n",
        "\n",
        "    # Add GRU layers\n",
        "    for i in range(config['gru_layers']):\n",
        "        model.add(GRU(config['hidden_units'], return_sequences=(i < config['gru_layers'] - 1)))\n",
        "        if activation_func == 'ReLU':\n",
        "            model.add(tf.keras.layers.ReLU())\n",
        "        elif activation_func == 'Leaky ReLU':\n",
        "            model.add(tf.keras.layers.LeakyReLU(alpha=0.01))\n",
        "        elif activation_func == 'Swish':\n",
        "            model.add(tf.keras.layers.Activation('swish'))\n",
        "        model.add(Dropout(config['dropout_rate']))\n",
        "\n",
        "    # Adding a output layer\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    optimizer = Adam(learning_rate=config['learning_rate'])\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Training the model \n",
        "    history = model.fit(X_train, y_train, epochs=config['epochs'], batch_size=128, validation_data=(X_test, y_test), verbose=0)\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    return history, test_loss, test_accuracy, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jpXIiNaKwYPM"
      },
      "outputs": [],
      "source": [
        "def display_predictions(model, X_test, y_test, original_texts, label_encoder, num_samples=5):\n",
        "    \"\"\"Display actual vs. predicted categories for a subset of the test set.\"\"\"\n",
        "    predictions = model.predict(X_test)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    for i in range(num_samples):\n",
        "        print(f\"Text: {original_texts[i][:200]}\")  # Show the first 200 characters of the text\n",
        "        print(f\"Actual Category: {label_encoder.inverse_transform([y_test[i]])[0]}\")\n",
        "        print(f\"Predicted Category: {label_encoder.inverse_transform([predicted_labels[i]])[0]}\")\n",
        "        print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "id": "ynPzKgukwaGw"
      },
      "outputs": [],
      "source": [
        "def plot_accuracy_loss(history, title):\n",
        "    \"\"\"Plot accuracy and loss curves for a given training history.\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'{title} - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{title} - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P7L31hxEyReg",
        "outputId": "7a94ba34-6cf5-419b-ac32-bb719daa7802"
      },
      "outputs": [],
      "source": [
        "for activation_func in ['ReLU', 'Leaky ReLU', 'Swish']:\n",
        "    print(f\"\\nTraining models with {activation_func} activation...\\n\")\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "    best_history = None\n",
        "\n",
        "    for config in all_configs:\n",
        "        print(f\"Training with configuration: {config}\")\n",
        "        history, loss, accuracy, model = train_and_evaluate(config, activation_func)\n",
        "        print(f\"Config: {config}, Test Accuracy: {accuracy:.4f}, Test Loss: {loss:.4f}\")\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_model = model\n",
        "            best_history = history\n",
        "\n",
        "    # Plotting  accuracy and loss for the best configuration\n",
        "    if best_history:\n",
        "        plot_accuracy_loss(best_history, f\"Best Model - {activation_func}\")\n",
        "\n",
        "    # Displaying predictions for the best model\n",
        "    if best_model:\n",
        "        display_predictions(best_model, X_test, y_test, X[:len(X_test)], label_encoder, num_samples=5)\n",
        "\n",
        "    print(f\"Best Test Accuracy for {activation_func}: {best_accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
