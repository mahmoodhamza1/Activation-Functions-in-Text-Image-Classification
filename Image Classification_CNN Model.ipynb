{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GKnj1niTmFHP",
        "outputId": "e2e36bf3-1df7-4b27-af43-5d3bf980e75c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "## Relu Funtion Code \n",
        "\n",
        "# Loading and preprocessing of CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "# Function to build deeper CNN model with ReLU function\n",
        "def build_deeper_relu_model(config):\n",
        "    model = Sequential()\n",
        "    for i in range(config['layers']):\n",
        "        model.add(Conv2D(config['filters'][i], (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3) if i == 0 else None))\n",
        "        model.add(BatchNormalization())\n",
        "        if i < config['layers'] - 1:  \n",
        "            model.add(MaxPooling2D((2, 2)))\n",
        "        else:\n",
        "            model.add(tf.keras.layers.GlobalAveragePooling2D())  \n",
        "        if 'dropout' in config:\n",
        "            model.add(Dropout(config['dropout']))\n",
        "    model.add(Dense(config['dense_units'], activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(learning_rate=config['lr']), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Plotting accuracy and loss curves\n",
        "def plot_curves(history, title):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    # Accuracy curves\n",
        "    ax1.plot(history['accuracy'], label='Training Accuracy')\n",
        "    ax1.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax1.set_title(f'{title} - Accuracy')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Loss curves\n",
        "    ax2.plot(history['loss'], label='Training Loss')\n",
        "    ax2.plot(history['val_loss'], label='Validation Loss')\n",
        "    ax2.set_title(f'{title} - Loss')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#  predictions vs actual labels\n",
        "def show_predictions(model, x_test, y_test, num_images=10):\n",
        "    predictions = model.predict(x_test[:num_images])\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    true_labels = np.argmax(y_test[:num_images], axis=1)\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
        "    for i in range(num_images):\n",
        "        axes[i].imshow(x_test[i])\n",
        "        axes[i].axis('off')\n",
        "        axes[i].set_title(f\"True: {true_labels[i]}\\nPred: {predicted_labels[i]}\")\n",
        "    plt.show()\n",
        "\n",
        "# Model configurations\n",
        "configs = {\n",
        "    'shallow': {'layers': 2, 'filters': [32, 64], 'dense_units': 128, 'dropout': 0.2, 'lr': 0.001, 'epochs': 10},\n",
        "    'medium':  {'layers': 3, 'filters': [32, 64, 128], 'dense_units': 256, 'dropout': 0.3, 'lr': 0.001, 'epochs': 20},\n",
        "    'deep':    {'layers': 4, 'filters': [32, 64, 128, 256], 'dense_units': 512, 'dropout': 0.4, 'lr': 0.0005, 'epochs': 30},\n",
        "    'deeper':  {'layers': 6, 'filters': [32, 64, 128, 256, 512, 512], 'dense_units': 1024, 'dropout': 0.5, 'lr': 0.0005, 'epochs': 30},\n",
        "}\n",
        "\n",
        "# Train, evaluate, and plot for ReLU\n",
        "history_dict = {}\n",
        "results = {}\n",
        "models = {}\n",
        "\n",
        "for name, config in configs.items():\n",
        "    print(f\"Training {name} configuration with ReLU activation...\")\n",
        "    model = build_deeper_relu_model(config)\n",
        "    history = model.fit(x_train, y_train, epochs=config['epochs'], batch_size=128, validation_split=0.2, verbose=2)\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    history_dict[name] = history.history\n",
        "    results[name] = {'test_accuracy': test_acc, 'test_loss': test_loss}\n",
        "    models[name] = model \n",
        "\n",
        "# Results\n",
        "print(\"\\nFinal Results:\")\n",
        "for config_name, metrics in results.items():\n",
        "    print(f\"{config_name} - Test Accuracy: {metrics['test_accuracy']:.4f}, Test Loss: {metrics['test_loss']:.4f}\")\n",
        "\n",
        "# Plot accuracy/loss curves for deep configuration\n",
        "print(\"\\nReLU Accuracy and Loss Curves for Deep Configuration\")\n",
        "plot_curves(history_dict['deep'], \"ReLU Deep Configuration\")\n",
        "\n",
        "# Show predictions for deep configuration\n",
        "print(\"\\nActual vs Predicted Outputs for ReLU (Deep Configuration)\")\n",
        "show_predictions(models['deep'], x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ReMQkL38pJo_",
        "outputId": "6afa59b3-dadf-4a2f-d40d-7c78679a4504"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "## Leaky ReLU function Code\n",
        "\n",
        "\n",
        "# Loading and preprocessing CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "# Function to build deeper CNN model with Leaky ReLU activation\n",
        "def build_deeper_leaky_relu_model(config):\n",
        "    model = Sequential()\n",
        "    for i in range(config['layers']):\n",
        "        model.add(Conv2D(config['filters'][i], (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "                         padding='same', input_shape=(32, 32, 3) if i == 0 else None))\n",
        "        model.add(BatchNormalization())\n",
        "        if i < config['layers'] - 1:  \n",
        "            model.add(MaxPooling2D((2, 2)))\n",
        "        else:\n",
        "            model.add(tf.keras.layers.GlobalAveragePooling2D())  \n",
        "        if 'dropout' in config:\n",
        "            model.add(Dropout(config['dropout']))\n",
        "    model.add(Dense(config['dense_units'], activation=tf.keras.layers.LeakyReLU(alpha=0.01)))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(learning_rate=config['lr']), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# configurations\n",
        "configs = {\n",
        "    'shallow': {'layers': 2, 'filters': [32, 64], 'dense_units': 128, 'dropout': 0.2, 'lr': 0.001, 'epochs': 10},\n",
        "    'medium':  {'layers': 3, 'filters': [32, 64, 128], 'dense_units': 256, 'dropout': 0.3, 'lr': 0.001, 'epochs': 20},\n",
        "    'deep':    {'layers': 4, 'filters': [32, 64, 128, 256], 'dense_units': 512, 'dropout': 0.4, 'lr': 0.0005, 'epochs': 30},\n",
        "    'deeper':  {'layers': 6, 'filters': [32, 64, 128, 256, 512, 512], 'dense_units': 1024, 'dropout': 0.5, 'lr': 0.0005, 'epochs': 30},\n",
        "}\n",
        "\n",
        "# Train and evaluate models for each configuration\n",
        "history_dict = {}\n",
        "results = {}\n",
        "models = {}\n",
        "\n",
        "for name, config in configs.items():\n",
        "    print(f\"Training {name} configuration with Leaky ReLU activation...\")\n",
        "    model = build_deeper_leaky_relu_model(config)\n",
        "    history = model.fit(x_train, y_train, epochs=config['epochs'], batch_size=128, validation_split=0.2, verbose=2)\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    history_dict[name] = history.history\n",
        "    results[name] = {'test_accuracy': test_acc, 'test_loss': test_loss}\n",
        "    models[name] = model\n",
        "\n",
        "#  Results\n",
        "print(\"\\nFinal Results:\")\n",
        "for config_name, metrics in results.items():\n",
        "    print(f\"{config_name} - Test Accuracy: {metrics['test_accuracy']:.4f}, Test Loss: {metrics['test_loss']:.4f}\")\n",
        "\n",
        "# Plot accuracy and loss curves\n",
        "def plot_curves(history, title):\n",
        "    epochs = range(1, len(history['accuracy']) + 1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Accuracy Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(epochs, history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'{title} - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history['loss'], label='Training Loss')\n",
        "    plt.plot(epochs, history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{title} - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function to display Actual vs Predicted Outputs\n",
        "def show_actual_vs_predicted(model, x_test, y_test, class_names, num_images=10):\n",
        "    y_pred = np.argmax(model.predict(x_test[:num_images]), axis=1)\n",
        "    y_true = np.argmax(y_test[:num_images], axis=1)\n",
        "\n",
        "    plt.figure(figsize=(12, 2))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(x_test[i])\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"True: {y_true[i]}\\nPred: {y_pred[i]}\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot Accuracy/Loss Curves for Leaky ReLU - Deep Configuration\n",
        "print(\"Leaky ReLU Accuracy and Loss Curves\")\n",
        "plot_curves(history_dict['deep'], \"Leaky ReLU Deep Configuration\")\n",
        "\n",
        "# Show Actual vs Predicted Outputs for Leaky ReLU - Deep Configuration\n",
        "print(\"\\nActual vs Predicted Outputs for Leaky ReLU\")\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "show_actual_vs_predicted(models['deep'], x_test, y_test, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c9r3IULj63wb",
        "outputId": "5906c00e-30b9-4c96-8ab0-6feec8eb9416"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "## Swish Function Code\n",
        "\n",
        "# Loading and preprocessing CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "# Function to build CNN model with Swish activation\n",
        "def build_deeper_swish_model(config):\n",
        "    model = Sequential()\n",
        "    for i in range(config['layers']):\n",
        "        model.add(Conv2D(config['filters'][i], (3, 3), activation=tf.keras.activations.swish, padding='same', input_shape=(32, 32, 3) if i == 0 else None))\n",
        "        model.add(BatchNormalization())\n",
        "        if i < config['layers'] - 1: \n",
        "            model.add(MaxPooling2D((2, 2)))\n",
        "        else:\n",
        "            model.add(tf.keras.layers.GlobalAveragePooling2D())  \n",
        "        if 'dropout' in config:\n",
        "            model.add(Dropout(config['dropout']))\n",
        "    model.add(Dense(config['dense_units'], activation=tf.keras.activations.swish))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(learning_rate=config['lr']), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Configurations for Swish activation \n",
        "configs = {\n",
        "    'shallow': {'layers': 2, 'filters': [32, 64], 'dense_units': 128, 'dropout': 0.2, 'lr': 0.001, 'epochs': 10},\n",
        "    'medium':  {'layers': 3, 'filters': [32, 64, 128], 'dense_units': 256, 'dropout': 0.3, 'lr': 0.001, 'epochs': 20},\n",
        "    'deep':    {'layers': 4, 'filters': [32, 64, 128, 256], 'dense_units': 512, 'dropout': 0.4, 'lr': 0.0005, 'epochs': 30},\n",
        "    'deeper':  {'layers': 6, 'filters': [32, 64, 128, 256, 512, 512], 'dense_units': 1024, 'dropout': 0.5, 'lr': 0.0005, 'epochs': 30},\n",
        "}\n",
        "\n",
        "# Training and evaluate models for each configuration\n",
        "swish_history_dict = {}\n",
        "swish_results = {}\n",
        "\n",
        "for name, config in configs.items():\n",
        "    print(f\"Training {name} configuration with Swish activation...\")\n",
        "    model = build_deeper_swish_model(config)\n",
        "    history = model.fit(x_train, y_train, epochs=config['epochs'], batch_size=128, validation_split=0.2, verbose=2)\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    swish_history_dict[name] = history.history\n",
        "    swish_results[name] = {'test_accuracy': test_acc, 'test_loss': test_loss}\n",
        "\n",
        "# Results\n",
        "print(\"\\nFinal Results:\")\n",
        "for config_name, metrics in swish_results.items():\n",
        "    print(f\"{config_name} - Test Accuracy: {metrics['test_accuracy']:.4f}, Test Loss: {metrics['test_loss']:.4f}\")\n",
        "\n",
        "# Accuracy and Loss Curves\n",
        "def plot_curves(history, title):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    # Accuracy\n",
        "    axs[0].plot(history['accuracy'], label='Training Accuracy')\n",
        "    axs[0].plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "    axs[0].set_title(f'{title} - Accuracy')\n",
        "    axs[0].set_xlabel('Epochs')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].legend()\n",
        "\n",
        "    # Loss\n",
        "    axs[1].plot(history['loss'], label='Training Loss')\n",
        "    axs[1].plot(history['val_loss'], label='Validation Loss')\n",
        "    axs[1].set_title(f'{title} - Loss')\n",
        "    axs[1].set_xlabel('Epochs')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot accuracy/loss curves for Swish\n",
        "print(\"Swish Accuracy and Loss Curves\")\n",
        "plot_curves(swish_history_dict['deep'], \"Swish Deep Configuration\")\n",
        "\n",
        "# Show Predicted vs Actual Outputs\n",
        "def show_predictions(model, x_test, y_test, num_images=10):\n",
        "    predictions = model.predict(x_test[:num_images])\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    true_labels = np.argmax(y_test[:num_images], axis=1)\n",
        "\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(x_test[i])\n",
        "        plt.title(f\"True: {true_labels[i]}\\nPred: {predicted_labels[i]}\")\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display predicted vs actual outputs for the deep configuration\n",
        "print(\"Swish Predicted vs Actual Outputs\")\n",
        "deep_swish_model = build_deeper_swish_model(configs['deep'])\n",
        "deep_swish_model.fit(x_train, y_train, epochs=configs['deep']['epochs'], batch_size=128, validation_split=0.2, verbose=0)\n",
        "show_predictions(deep_swish_model, x_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
